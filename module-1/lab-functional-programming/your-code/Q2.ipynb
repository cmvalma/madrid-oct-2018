{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to enhance the `get_bow_from_docs` function so that it will work with HTML webpages. In HTML, there are a lot of messy codes such as HTML tags, Javascripts, [unicodes](https://www.w3schools.com/charsets/ref_utf_misc_symbols.asp) that will mess up your bag of words. We need to clean up those junk before generating BoW.\n",
    "\n",
    "Next, what you will do is to define several new functions each of which is specialized to clean up the HTML codes in one aspect. For instance, you can have a `strip_html_tags` function to remove all HTML tags, a `remove_punctuation` function to remove all punctuation, a `to_lower_case` function to convert string to lowercase, and a `remove_unicode` function to remove all unicodes.\n",
    "\n",
    "Then in your `get_bow_from_doc` function, you will call each of those functions you created to clean up the HTML before you generate the corpus.\n",
    "\n",
    "Note: Please use Python string operations and regular expression only in this lab. Do not use extra libraries such as `beautifulsoup` because otherwise you loose the purpose of practicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your string handling functions below\n",
    "# Minimal 3 functions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_punctuation(txt):\n",
    "    \"\"\"\n",
    "    Removes all punctuation (like \",\" \".\", \":\") from the text entered, s\n",
    "        <DoLater> In the case of signs that can unite two words, as in\n",
    "        \"part-time\" or \"full-time\" (but not with a number as in \"9-week\"), \n",
    "        it doesn't delete.  \n",
    "    Argument/s: a string.\n",
    "    Return: string without punctuation.\n",
    "    \"\"\"\n",
    "    chars = \"`*#+/,;.:!&$_{}[]()\"\n",
    "    for char in chars:\n",
    "        # Beware! If we don't use the same 'txt' name at both sides of the assignmet, \n",
    "        # the var will remain with the value of last iteration.\n",
    "        txt = txt.replace(char, \"\")\n",
    "    # txt.re.findall( (w+[-_]\\w+), txt) # Problem with this one <continueLater>\n",
    "        # replace() doesnt't work with reg ex\n",
    "        txt = re.findall('[a-zA-Z][-_][a-zA-Z]', txt)\n",
    "    return txt\n",
    "\n",
    "# for c in \".,;-_:\":\n",
    "# corpus = corpus.append(f.read().replace(c, ''))\n",
    "\n",
    "def strip_html_tags(txt):\n",
    "    \"\"\"\n",
    "    Removes all <> tags from the text entered. \n",
    "    Argument/s: a string.\n",
    "    Return: stripped string.\n",
    "    \"\"\"\n",
    "    # txt = txt.replace(txt, [<*>], '')\n",
    "    # txt = txt.replace(txt, '<*>', '')\n",
    "    txt = txt.replace(txt, '<*>', '')\n",
    "    return txt\n",
    "    # Que no, que no quiere funcionar de ninguna forma\n",
    "\n",
    "def remove_numbers(txt):\n",
    "    \"\"\"\n",
    "    Removes all numbers. \n",
    "    Argument/s: a string.\n",
    "    Return: stripped string.\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-18-564a0f0acf63>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-564a0f0acf63>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    print(x)\u001b[0m\n\u001b[0m            \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "'''test\n",
    "x = strip_html_tags(\"asdf <pi> faso </p>\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-58c24ca7a438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"&*hola, cara-melo, _numo_, -miel, Agco, mello,. Hooa()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f275fbc7b654>\u001b[0m in \u001b[0;36mremove_punctuation\u001b[0;34m(txt)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Beware! If we don't use the same 'txt' name at both sides of the assignmet,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# the var will remain with the value of last iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;31m# txt.re.findall( (w+[-_]\\w+), txt) # Problem with this one <continueLater>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# replace() doesnt't work with reg ex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "'''test\n",
    "x = remove_punctuation(\"&*hola, cara-melo, _numo_, -miel, Agco, mello,. Hooa()\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "XXoladarieaspf\n",
      "o\n",
      "XXXXladarieaspf\n",
      "l\n",
      "XXXXXXadarieaspf\n",
      "a\n",
      "XXXXXXXXdXXrieXXspf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = \"holadarieaspf\"\n",
    "for letr in \"hola\":\n",
    "    print(letr)\n",
    "    x = x.replace(letr, 'XX')\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, paste your previously written `get_bow_from_docs` function below. Call your functions above at the appropriate place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_from_docs(docs, stop_words=[]):\n",
    "    \n",
    "    corpus = []\n",
    "    lDocsWords = [] #It was called lSenteWords (list of sentence words) in original exercise \n",
    "                    # as each text/doc contained one sentence.\n",
    "    duplicateNoStopWords = []    \n",
    "    bag_of_words = []\n",
    "    sente_freq = [] # almacena las frecuencias para la sentencia sobre la que se itera\n",
    "    term_freq = [] # aquí se volcarán las frecuencias de cada sentencia\n",
    "   \n",
    "\n",
    "    #Read and initial cleaning\n",
    "    for i in range(len(docs)):\n",
    "        with open(docs[i]) as f:\n",
    "            corpus.append(f.read().lower().replace('.', ''))\n",
    "\n",
    "            \n",
    "\n",
    "    # Break list of strings into lists of words of each document:\n",
    "    for i in range(len(corpus)):\n",
    "        lDocsWords.append(re.findall('\\w+', corpus[i]))\n",
    "    \n",
    "        # Enter each list of words element into sigle list of words, without stop words (no duplicates removal)\n",
    "        for word in lDocsWords[i]:\n",
    "            if not word in stop_words: \n",
    "                duplicateNoStopWords.append(word)       \n",
    "\n",
    "                \n",
    "    # Get single elements:       \n",
    "    bag_of_words = set(duplicateNoStopWords)    \n",
    "                          \n",
    "\n",
    "        \n",
    "    # For each doc (i.e. the number of the doc)...\n",
    "    for numSente in range(len(lDocsWords)):\n",
    "        # ... we compare if each if its words is in the BOW:\n",
    "        for word in bag_of_words:\n",
    "            # Count the number of times the word appears in the text:\n",
    "            freqNum = lDocsWords[numSente].count(word)\n",
    "            sente_freq.append(freqNum)\n",
    "        # Add each of the doc lists to term_freq:\n",
    "        term_freq.append(sente_freq)\n",
    "        # Reset the frequency for the sentence, as we start another one:\n",
    "        sente_freq = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"bag_of_words\": bag_of_words,\n",
    "        \"term_freq\": term_freq\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, read the content from the three HTML webpages in the `your-codes` directory to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "bow = get_bow_from_docs([\n",
    "        'www.coursereport.com_ironhack.html',\n",
    "        'en.wikipedia.org_Data_analysis.html',\n",
    "        'www.lipsum.com.html'\n",
    "    ],\n",
    "    stop_words.ENGLISH_STOP_WORDS\n",
    ")\n",
    "\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any problem in the output? How do you improve the output?\n",
    "\n",
    "A good way to improve your codes is to look into the HTML data sources and try to understand where the messy output came from. A good data analyst always learns about the data in depth in order to perform the job well.\n",
    "\n",
    "Spend 20-30 minutes to improve your functions or until you feel you are good at string operations. This lab is just a practice so you don't need to stress yourself out. If you feel you've practiced enough you can stop and move on the next challenge question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
